

\section{Methods}



\subsection{Performing STAR Alignment}

\subsubsection{Generating Genome Indexes Files}

STAR genomes are available for a limited number of genomes on the Gingeras lab. We will use the \textit{Bos taurus} UMD3.1.87 annotations file since the UMD 3.1.1/bosTau8 version of the genome was used in \citep{Ariel2021}. The GTF file describes all exons whereas the .dna.toplevel.fa fasta file contains the corresponding sequences.



\begin{verbatim}
wget http://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/\
STAR/STARgenomes/Old/ENSEMBL/bos_taurus/Bos_taurus.UMD3.1.87.gtf;

wget http://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/\
STAR/STARgenomes/Old/ENSEMBL/bos_taurus/Bos_taurus.UMD3.1.dna.toplevel.fa;
\end{verbatim}

To prepare genome index files for STAR, use the genomeGenerate built-in STAR command.
Note that you have to create a directory where STAR could buid the index:

\begin{verbatim}
mkdir Bos_taurus.UMD3.1.87_index
\end{verbatim}

Then, build the index with:

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -pe smp 6 star_genome_index_builder.sh
\end{verbatim}


\noindent star\_genome\_index\_builder.sh:
\begin{verbatim}
#! /bin/bash
#$ -N 'STAR_genome_builder'
#$ -o ./star_genome_builder_log.txt

STAR --runThreadN 6 \
--runMode genomeGenerate \
--genomeDir Bos_taurus.UMD3.1.87_index \
--genomeFastaFiles ./Bos_taurus.UMD3.1.dna.toplevel.fa \
--sjdbGTFfile ./Bos_taurus.UMD3.1.87.gtf \
--sjdbOverhang 99
\end{verbatim}



\subsubsection{Aligning RNAseq reads (1st mapping pass)}

At this step, we align our raw .fastq files with the STAR aligner.

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 4 star_aligner.sh
\end{verbatim}


\noindent star\_aligner.sh:
\begin{verbatim}
#!/bin/bash
#$ -N 'STAR_aligner'
#$ -o STAR_aligner_1st_pass.$TASK_ID.log

input=$(head -n $SGE_TASK_ID SRR.list | tail -n 1)

mkdir -p ../analysis/STAR/output/$input

STAR --genomeDir ./Bos_taurus.UMD3.1.87_index \
--runThreadN 12 \
--readFilesIn  ./$input/$input"_1.fastq" ./$input/$input"_2.fastq" \
--outFileNamePrefix ../analysis/STAR/output/$input \
--outSAMtype BAM SortedByCoordinate \
--outSAMunmapped Within \
--outSAMattributes Standard
\end{verbatim}



\subsubsection{Aligning RNAseq reads (2nd mapping pass)}

The current recommendation of the GATK Per-sample RNAseq  workflow is to perform 2-pass mapping with the STAR aligner. The 2nd mapping pass is identical to the first alignment except that all splice junctions discovered in the first pass are given as input to the programs, allowing to detect more reads mapping to novel junctions  \cite{Dobin2013}. Simply specify a list of all splice junctions files after the --sjdbFileChrStartEnd argument.


\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 4 star_aligner_2nd_pass.sh
\end{verbatim}

\noindent star\_aligner\_2nd\_pass.sh listing
\begin{verbatim}
#!/bin/bash
#$ -N 'STAR_aligner_2nd_pass'
#$ -o STAR_aligner_2nd_pass.$TASK_ID.log

input=$(head -n $SGE_TASK_ID SRR.list | tail -n 1)

mkdir -p ../analysis/STAR/output_2nd_pass/$input

STAR --genomeDir ./Bos_taurus.UMD3.1.87_index \
--runThreadN 4 \
--readFilesIn  ./$input/$input"_1.fastq" ./$input/$input"_2.fastq" \
--outFileNamePrefix ../analysis/STAR/output_2nd_pass/$input \
--outSAMtype BAM SortedByCoordinate \
--outSAMunmapped Within \
--outSAMattributes Standard \
--outTmpDir ../analysis/STAR/output_2nd_pass/_STARtmp_$SGE_TASK_ID \
--sjdbFileChrStartEnd \
../analysis/STAR/output/SRR5487372SJ.out.tab \
../analysis/STAR/output/SRR5487384SJ.out.tab \
...
../analysis/STAR/output/SRR5487430SJ.out.tab \
../analysis/STAR/output/SRR5487442SJ.out.tab
\end{verbatim}



\subsubsection{Sorting Alignment Files}


After you align your sequences. You will want that your .bam files to be indexed and sorted by coordinates. As mentioned earlier, STAR allows the output to be sorted by coordinates. If you have carefully followed this tutorial, there is no need to sort again your bam files and you can safely ignore the next step and jump to the next subheading. However, if for any reasons, your aligmnents files are not sorted adequately, the sorting by coordinates with the Samtools is as easy as :

\begin{verbatim}
samtools sort file.bam -o file_sorted.bam
\end{verbatim}

In a folder containing unsorted alignments, first prepare a list of bam files to be sorted with:

\begin{verbatim}
ls -1 *.bam > bam.list
\end{verbatim}

For our data, a qsub script we could re-sort our alignments files with:


\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 2 samtools_sort_bam.sh
\end{verbatim}

The following listing could also be used as a template to perform a variety of Samtools subcommands that involve renaming the output files.

\noindent samtools\_sort\_bam.sh
\begin{verbatim}
#!/bin/bash
#$ -V
#$ -N samtools_sort
#$ -o samtools_sort.$TASK_ID.log

input=$(head -n $SGE_TASK_ID bam.list | tail -n 1 | xargs basename -s '.bam')

samtools sort $input.bam -o $input"_sorted.bam"

\end{verbatim}


\subsubsection{Indexing Alignment Files}

BAM files (the binary analog of Sequence Alignment Mapping (SAM) files) are very efficient bioinformatic files designed to store high-throughput alignment data. Typically, they contain the result of the alignment of millions of sequencing reads against a reference genome and greatly benefit from being indexed by genomic positions for faster random access to the aligned reads at a specific locus. In practice most programs will not run in the absence of index files. Obviously, BAM files need to be sorted before being indexed. To index all bam files present in the STAR output\_2nd\_pass folder, first prepare a list of bam files to be indexed with:

\begin{verbatim}
ls -1 *.bam > bam.list
\end{verbatim}

Then, you can index all alignements in parallel with:

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 2 samtools_batch_index.sh
\end{verbatim}

\noindent samtools\_batch\_index.sh

\begin{verbatim}
#!/bin/bash
#$ -N samtools_index
#$ -o samtools_index.$TASK_ID.log

input=$(head -n $SGE_TASK_ID bam.list | tail -n 1)

samtools index $input
\end{verbatim}





\subsection{Adding Read Groups}

Contrarily to FASTQ files, SAM files have the capacity to handle large amount of metadata. A good practice would be to always include informations about the reference genome and the samples in aligment files. Informations about the processing steps could also be stored. Many programs, such as GATK, will automatically add this kind of metadata information when manipulating SAM/BAM files.


Most GATK commands involving BAM files require that read groups have been defined. This step ensure that relevant informations about the sequencing process follow each read in downstream analyses. In addition, when this step is done carefully, it allow the Base recalibration step to mitigate the consequences of the sequencing biais that might arise due to how the sequencing process was performed \cite{GATK_ReadGroups}. For example, when samples are multiplexed, it is important to known which reads origin from the same library, which were sequenced on the same flowcell, etc. The GATK engine requires the presence of several read group fields to run without errors. To learn more about the Read groups as understand by the GATK team and to learn how to derive this information from read names, one should consult this page: \href{https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups}{https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups}.

Here, we will set the flowcells, sequencing lanes and sample barcode in the PU (Platform Unit) tag. We will also set the PL (Platform/technology used to produce the read) and LB (DNA preparation library identifier) tags. Note that the ID (Read group identifier) tag is overrided by the PU tag for base recalibration when the latter is defined.

Before running the main script, we will extract the first and fifth columns from our metadata file and place them in separate files, namely RGLB.txt and  RGPU.txt, that will be further used to populate the LB and PU read groups fields:

\begin{verbatim}
# Corresponding to the sample name
cut -f 1 -d ',' ../../metadata/metadata.txt | tail -n +2 > RGLB.txt

# Corresponding to the PU tag FLOW CELL 
cut -f 5 -d ',' ../../metadata/metadata.txt | tail -n +2 > RGPU.txt
\end{verbatim}

where /metadata/metadata.txt looks like:

\begin{verbatim}
sample,cowID,SRAID,Run,RGPU
A_CTL24,cowA,SRS2153774,SRR5487372,C5NL3ACXX.1.CAGATC
A_MAP24,cowA,SRS2153779,SRR5487376,C5NL3ACXX.1.TGACCA
B_CTL24,cowB,SRS2153781,SRR5487378,C5NL3ACXX.3.TGACCA
B_MAP24,cowB,SRS2153786,SRR5487382,C5NL3ACXX.3.GTGAAA
C_CTL24,cowC,SRS2153787,SRR5487384,C5K8FACXX.3.AGTCAA
C_MAP24,cowC,SRS2153791,SRR5487388,C5K8FACXX.3.GTCCGC
D_CTL24,cowD,SRS2153793,SRR5487390,C5K8FACXX.1.TGACCA
...
K_CTL24,cowK,SRS2153835,SRR5487432,C547FACXX.5.AGTCAA
K_MAP24,cowK,SRS2153839,SRR5487436,C547FACXX.5.GTCCGC
L_CTL24,cowL,SRS2153841,SRR5487438,C547FACXX.7.AGTCAA
L_MAP24,cowL,SRS2153845,SRR5487442,C547FACXX.7.GTCCGC
\end{verbatim}


The RGLB.txt file will therefore hold the library name for each 24 samples (e.g. C\_MAP24, which design a RNAseq library done on the cow C 24h post-infection with the MAP pathogen). The RGPU.txt hold the Platform Unit tag, which is made of three types of informations: the flowcell, the sequencing lane and the barcode identifiers, with the following format: {FLOWCELL\_BARCODE}.{LANE}.{SAMPLE\_BARCODE}. 


Use the following command to add read groups in alignments files:

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 1 AddOrReplaceReadGroups.sh
\end{verbatim}

In the following listing, note that we use the value given by \$SGE\_TASK\_ID -1 to fetch the correct values in RGLB and RGPU arrays because Bash arrays are zero-indexed:

\noindent AddOrReplaceReadGroups.sh:
\begin{verbatim}
#!/bin/bash
#$ -N AddOrReplaceReadGroups
#$ -o logfile.$TASK_ID.log

eval "$(conda shell.bash hook)"
conda activate gatk4

SAMPLES="$HOME/jsb/springer2/RNAseq_GATK_JGW/metadata/samples.list"
BAMPATH="$HOME/jsb/springer2/RNAseq_GATK_JGW/analysis/STAR/output_2nd_pass"
OUTPUT="$HOME/jsb/springer2/RNAseq_GATK_JGW/analysis/addReadGroups"

readarray -t RGLB < ./RGLB.txt
readarray -t RGPU < ./RGPU.txt

input=$(head -n $SGE_TASK_ID $SAMPLES | tail -n 1)

java -jar $PICARD AddOrReplaceReadGroups \
I=$BAMPATH/$input"Aligned.sortedByCoord.out.bam" \
O=$OUTPUT/$input".bam" \
RGLB=${RGLB[$SGE_TASK_ID -1]} \
RGPL=ILLUMINA \
RGPU=${RGPU[$SGE_TASK_ID -1]} \
RGID=${RGPU[$SGE_TASK_ID -1]} \
RGSM=$input

conda deactivate
\end{verbatim}

After completion of the previous step, we will want to validate that the reads groups have been correctly set. When present, read groups are written in the header of BAM files. Thus, this simple UNIX trick allows one to quickly inspect the read groups associated to a BAM file:

\begin{verbatim}
samtools view -H sample.bam | grep '@RG'	
\end{verbatim}


To iterate on all our .bam files, use something like:

\begin{verbatim}
for i in `ls *.bam | xargs basename -s '.bam'`;\
do echo $i; samtools view -H $i.bam | grep '@RG'; done
\end{verbatim}

\begin{verbatim}
SRR5487372
@RG	ID:C5NL3ACXX.1.CAGATC	LB:A_CTL24	PL:ILLUMINA	SM:SRR5487372	PU:C5NL3ACXX.1.CAGATC
SRR5487376
@RG	ID:C5NL3ACXX.1.TGACCA	LB:A_MAP24	PL:ILLUMINA	SM:SRR5487376	PU:C5NL3ACXX.1.TGACCA
SRR5487378
@RG	ID:C5NL3ACXX.3.TGACCA	LB:B_CTL24	PL:ILLUMINA	SM:SRR5487378	PU:C5NL3ACXX.3.TGACCA
SRR5487382
@RG	ID:C5NL3ACXX.3.GTGAAA	LB:B_MAP24	PL:ILLUMINA	SM:SRR5487382	PU:C5NL3ACXX.3.GTGAAA
\end{verbatim}





\subsection{Marking Duplicates Reads}


Duplicate reads can arise from PCR duplication artifacts that take place during the library construction or from reading errors that occur during the sequencing process (optical duplicates). Regardless of their origin, these reads need to be identified in alignment files. The MarkDuplicate program from the Picard tools have many options to deal with these issue and output some metrics. For example, the program offers the possibility to completely remove the duplicate reads and to make the distinction between optical and PCR duplicates.

Here we will simply identified the duplicate reads in our .bam files before proceeding to the next step:

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 2 MarkDuplicates.sh
\end{verbatim}


\noindent MarkDuplicates.sh:
\begin{verbatim}
#!/bin/bash
#$ -N MarkDuplicates
#$ -o logfile.$TASK_ID.log

SAMPLES="$HOME/jsb/springer/metadata/samples.list"
BAMPATH="$HOME/jsb/springer/analysis/addReadGroups"
OUTPUT="$HOME/jsb/springer/analysis/MarkDuplicates"

input=$(head -n $SGE_TASK_ID $SAMPLES | tail -n 1)

java -jar $PICARD MarkDuplicates \
I=$BAMPATH/$input".bam" \
O=$OUTPUT/$input"_marked_duplicates.bam" \
M=$OUTPUT/$input"_marked_dup_metrics.txt"
\end{verbatim}

\subsection{Splitting RNAseq reads}

With the non-GATK alignment of raw sequences, the SplitNCigarReads step, (formerly called SplitNTrim step in previous versions of GATK workflows), is highly specific to RNAseq analysis. During this step reads that span splice junctions (for example reads that align over distinct exons) are split in smaller reads during the process. This step will produce alignments files that will adequately represent reads that span splice junctions. However, before doing this step, we need to get and prepare reference genome files.


\subsubsection{Preparing Reference Genome Files}

Until now, we have work with STAR reference genome which barely contains sequences that are transcribed in RNA. In contrast, the GATK SplitNCigarReads command requires to use a complete reference sequence. At this point, one should be advised to download the same version of the refence genome that was used to prepare the STAR genome.

To get the UMD3.1 assembly, visit \href{https://bovinegenome.elsiklab.missouri.edu/}{https://bovinegenome.elsiklab.missouri.edu/} and download the UMD3.1\_chromosomes.fa.gz and Ensembl79\_UMD3.1\_genes.gff3.gz files.


It is crucial that entries in the reference genome match the corresponding ones in the STAR genome. If you inspect the fasta headers of the UMD3.1\_chromosomes.fa, you will notice that each fasta entry contains many fields (e.g. gnl , UMD3.1 Accession numbers):

\begin{verbatim}
grep ">" UMD3.1_chromosomes.fa
\end{verbatim}


\begin{verbatim}
>gnl|UMD3.1|GK000010.2 Chromosome 10 AC_000167.1
>gnl|UMD3.1|GK000011.2 Chromosome 11 AC_000168.1
>gnl|UMD3.1|GK000012.2 Chromosome 12 AC_000169.1
...
>gnl|UMD3.1|GJ060407.1 GPS_000344847.1 NW_003101152.1
>gnl|UMD3.1|GJ060408.1 GPS_000344848.1 NW_003101153.1
\end{verbatim}

In contrast, the chromosomes entries in the STAR genome are quite different:

\begin{verbatim}
cat Bos_taurus.UMD3.1.87_index/chrName.txt | head -n 34
\end{verbatim}

**SHOW THE RESULT**

Therefore, the chromosome identifiers found in the \textit{Bos taurus} UMD3.1 genome need to be changed to match their counterparts in the STAR genome. This could be achieved with UNIX SED:

\begin{verbatim}
sed -r s'/^>.+Chromosome\s+(\S+)\s+.+/>\1/' UMD3.1_chromosomes.fa > temp1.fa
grep ">" temp1.fa | head -n 40
sed -r s'/^>gnl\|UMD3\.1\|(\S+)+\s+.+/>\1/' temp1.fa > temp2.fa
grep ">" temp2.fa | tail -n +30 | head -n 10
mv temp2.fa refGenome.fasta
rm temp1.fa
\end{verbatim}


Along with the reference genome, many GATK tools will need a dictionary file ending in .dict and an index file ending in .fai.\href{https://gatk.broadinstitute.org/hc/en-us/articles/360035531652-FASTA-Reference-genome-format}{ https://gatk.broadinstitute.org/hc/en-us/articles/360035531652-FASTA-Reference-genome-format}). These files need to share the same basename as the reference genome. You can prepare the index with:

\begin{verbatim}
samtools faidx refGenome.fasta
\end{verbatim}

On the other hand, use the GATK CreateSequenceDictionary tool to create the required .dict file:

\begin{verbatim}
gatk CreateSequenceDictionary -R refGenome.fasta
\end{verbatim}




\subsubsection{Running SplitNCigarReads}

Having prepared reference genome files and index, we are ready to lauch the following SplitNCigarRead listing:


\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 4  SplitNCigarReads.sh
\end{verbatim}



\begin{verbatim}
#!/bin/bash
#$ -N SplitNCigarReads
#$ -o SplitNCigarReads.$TASK_ID.log
	
eval "$(conda shell.bash hook)"
conda activate gatk4
	
SAMPLES="$HOME/jsb/springer/metadata/samples.list"
BAMPATH="$HOME/jsb/springer/analysis/MarkDuplicates"
OUTPUT="$HOME/jsb/springer/analysis/SplitNCigarReads"
	
input=$(head -n $SGE_TASK_ID $SAMPLES | tail -n 1)

gatk SplitNCigarReads \
-R refGenome.fasta \
-I $BAMPATH/$input"_marked_duplicates.bam" \
-O $OUTPUT/$input"_SplitNCigarReads.bam" \
--tmp-dir $output/gatk_tmp
	
conda deactivate
\end{verbatim}



\subsection{Performing Base Quality Score Recalibration}

Base Quality Score Recalibration (BQSR) is an optional, but highly recommended step, that figure in DNAseq and RNAseq GATK best practices workflows. This complex procedure, which involves binning and machine learning consist of re-evaluating all base quality scores assigned by sequencing machines which are prone to systematic technical errors \cite{GATK_BaseQuality}. This procedure lead to more accurate base calls, a situation that improve the accuracy of variant calling overall. More details about how the model is build and how new quality scores are computed can be found at \href{https://gatk.broadinstitute.org/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR-}{https://gatk.broadinstitute.org/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR-}.

\subsubsection{Using BaseRecalibrator}

In the first key step  a model of covariation is build with two components: the alignement given as input (BAM files) and a set of known variants (VCF file). The BaseRecalibrator will then produce a recalibration file which is used in the second key step of the procedure. To perform this task one would need a source of reliable variants in the investigated genome. With human data and other common model organisms, it would be easy to find such sources of variants, thanks to the existence of variation databases. Here we use a single VCF file containing the results of the BovineSNP50 genotyping BeadChip to train the model, but you can feed the model with several VCF files.



\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 4  BaseRecalibrator_first_step.sh
\end{verbatim}



%BaseRecalibrator first script
\begin{verbatim}
#!/bin/bash
#$ -N BaseRecalibrator
#$ -o BaseQualityRecalibration_$TASK_ID.log

eval "$(conda shell.bash hook)"
conda activate gatk4

SAMPLES="$HOME/jsb/springer/metadata/samples.list"
BAMPATH="$HOME/jsb/springer/analysis/SplitNCigarReads"
OUTPUT="$HOME/jsb/springer/analysis/BaseQualityRecalibration"

input=$(head -n $SGE_TASK_ID $SAMPLES | tail -n 1)

gatk BaseRecalibrator \
-R ./refGenome.fasta \
-I $BAMPATH/$input"_SplitNCigarReads.bam" \
--known-sites ./known.vcf \
-O $OUTPUT/$input"_recal_data.table"

conda deactivate
\end{verbatim}





\subsubsection{Using ApplyBQSR to Adjust the Scores}

In the second step of the BaseQuality recalibration process, the GATK ApplyBQSR command assigns new scores to each base based on the model produced in the first step and produces new BAM files that reflet these changes.  

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 4  ApplyBQSR_second_step.sh
\end{verbatim}


%ApplyBQSR listing
\begin{verbatim}
#!/bin/bash
#$ -N ApplyBQSR
#$ -o ApplyBQSR_$TASK_ID.log

eval "$(conda shell.bash hook)"
conda activate gatk4

SAMPLES="$HOME/jsb/springer/metadata/samples.list"
BAMPATH="$HOME/jsb/springer/analysis/SplitNCigarReads"
OUTPUT="$HOME/jsb/springer/analysis/BaseQualityRecalibration"

echo $SAMPLES
input=$(head -n $SGE_TASK_ID $SAMPLES | tail -n 1)

gatk ApplyBQSR \
-R ./refGenome.fasta \
-I $BAMPATH/$input"_SplitNCigarReads.bam" \
--bqsr-recal-file $OUTPUT/$input"_recal_data.table" \
-O $OUTPUT/$input"_recal.bam"

conda deactivate
\end{verbatim}

\subsubsection{Comparing pre- and post-recalibration metrics}

To visualize the effect of the procedure, one can build a seconde model and generate plots with data before and after recalibration \cite{GATK_BaseQuality} The GATK command AnalyzeCovariates was created for that purpose as in the following example:

\begin{verbatim}
gatk AnalyzeCovariates \
-before recal1.table \
-after recal2.table \
-plots AnalyzeCovariates.pdf
\end{verbatim}



\subsection{Joint Genotyping Variant Calling}


The Base recalibration being the final step in the Data cleanup part of the workflow present here (figure 1), we are ready for discovering variants from our analysis-ready RNAseq reads with the joint genoyping approach. Again, this procedure involves several steps which are part of the Germline short variant discovery (SNPs + Indels) Best Practices Workflows \cite{GATK_BP_Germline}. First, we will call variants per-sample with the HaplotypeCaller engine, then we will merge GVCF files with the GenomicsDBImport tool and finally we will perform joint genotyping with the GATK GenotypeGVCFs command.




\subsubsection{Calling Variants Per-sample (GVCF mode)}
%GCVF listing


In this step, the GATK variant calling engine, HaplotypeCaller, identifies candidate variation sites and record them in GVCF files. This is why this step has been called "GVCF workflow". GVCF files act as intermediate between analysis ready reads (BAM files) and the final joint analysis of multiple samples \cite{GATK_jointCalling_1}. Such intermediate files facilitate the incremental discovery of variants and should be kept since they can be reused in novel rounds of joint genotyping analysis. Our listing to produce GVCF files for each sample reads as follow:

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -t 1-24 -pe smp 4  singleSampleGVCFcalling.sh
\end{verbatim}

singleSampleGVCFcalling.sh:

\begin{verbatim}
#!/bin/bash
#$ -N GenotypeGVCF
#$ -o GenotypeGVCF.log

eval "$(conda shell.bash hook)"
conda activate gatk4

SAMPLES="$HOME/jsb/springer/metadata/samples.list"
BAMPATH="$HOME/jsb/springer/analysis/BaseQualityRecalibration"
OUTPUT="$HOME/jsb/springer/analysis/GVCF"

echo $SAMPLES
input=$(head -n $SGE_TASK_ID $SAMPLES | tail -n 1)

gatk --java-options "-Xmx4g" HaplotypeCaller \
‐dontUseSoftClippedBases \​
‐stand_call_conf 20 \
‐stand_emit_conf 20 \
-R ./refGenome.fasta \
-I $BAMPATH/$input"_recal.bam" \
-O $OUTPUT/$input".g.vcf.gz" \
-ERC GVCF

conda deactivate
\end{verbatim}



\subsubsection{Merging of GCVF files}

Merging of GCVF files has been revised recently by GATK experts by the addition of the novel GATK GenomicsDBImport command. It resembles the former CombineGVCFs command, but promises better performances for parsing of VCF files and computing values stored in the VCF INFO fields. More details can be found at \href{https://gatk.broadinstitute.org/hc/en-us/articles/360036883491-GenomicsDBImport}{https://gatk.broadinstitute.org/hc/en-us/articles/360036883491-GenomicsDBImport}. Interestingly, under the hood, this command make use of a novel data managament system called  TileDB which is particluarly useful for representing huge sparse 2D arrays such as the genomic data found in VCF files \cite{Papadopoulos2016}. 


The command requires a map files, here cohort\_1.sample\_map, that list the full paths of all GVCF files to be included in the resulting database. 

The -L option specifies the range genomic intervals on which the command operate and may be used several times. Since, we are interested to call variants on all bovine autosomes, we used it many times in the following listing:

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -pe smp 12  GenomicsDBImport.sh
\end{verbatim}

%GenomicsDBImport listing
\begin{verbatim}
#!/bin/bash
#$ -N GenomicsDBImport
#$ -o GenomicsDBImport.log	
	
eval "$(conda shell.bash hook)"
conda activate gatk4

SAMPLES="$HOME/jsb/springer/metadata"
OUTPUT="$HOME/jsb/springer/analysis/GenomicsDBImport"

gatk --java-options "-Xmx4g -Xms4g" \
GenomicsDBImport \
--genomicsdb-workspace-path $OUTPUT/"my_database" \
-L 1 \
-L 2 \
...
-L 28 \
-L 29 \
--sample-name-map $SAMPLES/"cohort_1.sample_map" \
--tmp-dir $HOME/jsb/tmp \
--reader-threads 5

conda deactivate

\end{verbatim}


\subsubsection{Joint Genotyping}

The final step in this variant calling workflow, which is actually the joint genotyping step, is performed with the GATK GenotypeGVCFs command (  \href{https://gatk.broadinstitute.org/hc/en-us/articles/360037057852-GenotypeGVCFs}{https://gatk.broadinstitute.org/hc/en-us/articles/360037057852-GenotypeGVCFs}). During this operation, all samples for which variant have been pre-called with HaplotypeCaller are jointly genotyped. This step could be rerun when new samples are available, allowing the so-called incremental discovery of variants.


The command take as argument a GenomicsDB workspace created by GenomicsDBImport, which can be seen as a sophisticated database that facilitate the parsing of the genomic data present in GVCF files. Alternatively, a single multi-sample GCVF file produced by CombineGVCFs (used after HaplotypeCaller) can also be given as input.

\begin{verbatim}
qsub -V -S /bin/bash -cwd -j y -pe smp 12  GenotypeGVCF.sh
\end{verbatim}


%GenotypeGVCFs listing
\begin{verbatim}
#$ -N GenotypeGVCF
#$ -o GenotypeGVCF.log

eval "$(conda shell.bash hook)"
conda activate gatk4

gatk --java-options "-Xmx4g" GenotypeGVCFs \
	 -R refGenome.fasta \
	 -V gendb://my_database \
	 -O output.vcf.gz

conda deactivate
\end{verbatim}




\subsection{Querying variants}


As mentionned earlier, BCFtools are optimized by design, to query and manipulate VCF files. Therefore, it is worth the pain to familiarize with these tools rather than using the plain VCF files with UNIX tricks. In addition, since BCFtools work well with compressed files it would be a waste of time to compress or extract VCF files not to mention the risk of corrupting or erasing a VCF file. Because one can easily erase a file by mistakes when attempting to redirect the result of querying and filtering commands, it is a good practice to keep a copy of the primary VCF output file (output.vcf.gz) in a safe place.

That said, the BCFtools command are highly versatile and can be used in several circumstances ranging from performing simples queries to applying complex filters. Here a few examples:

To get a list of all samples present in a VCF files:

\begin{verbatim}
bcftools query -l output.vcf.gz
\end{verbatim}

To get useful metrics, like the sequencing depth or the ratio of transitions and transversions use the built-in stats command:
\begin{verbatim}
bcftools stats output.vcf.gz  > stats.txt
\end{verbatim}

To quickly see the numbers of variants that fall in each category:
\begin{verbatim}
bcftools stats output.vcf.gz | grep -E "^SN"
\end{verbatim}

To mask the VCF header and see variants from selected samples in a specific regions:
\begin{verbatim}
bcftools view -H -s SRR5487378,SRR5487390 -r 1:1-100000 output.vcf.gz
\end{verbatim}


The query command make possible to fetch some VCF columns and manipulate them easily. For example, the command:

\begin{verbatim}
bcftools query -i 'QUAL>20 && INFO/DP>10' \
	           -f'chr%CHROM pos:%POS Q:%QUAL DP=%INFO/DP\n' \
output.vcf.gz | head -n 10
\end{verbatim}

give the following result:
\begin{verbatim}
chr1 pos:242647 Q:2073.95 DP=46
chr1 pos:242649 Q:2073.95 DP=46
chr1 pos:245489 Q:31.59 DP=12
chr1 pos:246301 Q:75.98 DP=15
chr1 pos:246448 Q:190.97 DP=15
chr1 pos:247141 Q:151.58 DP=11
chr1 pos:247219 Q:66.88 DP=12
chr1 pos:247224 Q:66.88 DP=12
chr1 pos:247245 Q:98.22 DP=13
chr1 pos:248413 Q:67.87 DP=22
\end{verbatim}






\subsection{Filtering variants}





Contrarily to DNAseq workflows where sophisticated methods for filtering variants are available (VQSR and CNNScoreVariants), the current recommandation for RNAseq is to use hard filters as described in the canonical RNAseq Best practice RNAseq short variant discovery workflow \cite{GATK_RNAseq_variant_discovery}. The GATK and Picard tools have several commands to filter variants although the BCFtools filter and view commands could also be used. To perform hard filter, one could use the GATK VariantFiltration command  (\href{https://gatk.broadinstitute.org/hc/en-us/articles/360037269391-VariantFiltration}{https://gatk.broadinstitute.org/hc/en-us/articles/360037269391-VariantFiltration}) which is designed to filter variants on the basis of annotations present in INFO or FORMAT VCF fields.

Naturally, the extend of filtration steps that could be done depends on the nature of the downstream analysis that will be performed. Filtering variants is an active aera of research and the reader is invite to consult the GATK Best practices workflows documentation to be aware of all the novelties.

Among the numerous filtering that could be done, we present here a filtering step specific to RNAseq data that was proposed by GATK experts in previous Best practices workflows, but that should be still appropriate. It consist of filtering out clusters of at least 3 SNPs that are within a window of 35 bases between them.


\begin{verbatim}
gatk VariantFiltration \
-R refGenome.fasta \
-V output.vcf.gz \
-O filtered.vcf.gz \
--cluster-window-size 35 \
--cluster-size 3
\end{verbatim}

The GATK command write PASS or SnpCluster in the FILTER field (rather than the dot that was present in VCF file given as input) and add the following lines to the output VCF header:

\begin{verbatim}
##FILTER=<ID=SnpCluster,Description="SNPs found in clusters">
##FILTER=<ID=PASS,Description="All filters passed">
\end{verbatim}


Thus, to actually get rid off these variants, use the BCFtools view command with the --apply-filter option:
\begin{verbatim}
bcftools view --apply-filters .,PASS filtered.vcf.gz | bgzip -c > purged.vcf.gz
\end{verbatim}


Note how the BCFtools are fully integrated with other UNIX commands. Here the filtered VCF files is given to the HSTlib bgzip program that compress it before that the UNIX file redirection operator write the result to a new file. You can validate that SNP clusters do not appear in the purged.vcf.gz with:


\begin{verbatim}
bcftools view purged.vcf.gz | grep SnpCluster
\end{verbatim}

In DNA-seq, the generic current recommendation is to filter out variants based on Fisher Strand values (FS > 60.0) and Quality by depth values (QD < 2.0).

At first, we could be interested to know how many variants will be excluded with FS > 60.0:

\begin{verbatim}
bcftools filter -i 'FS > 60' purged.vcf.gz | bcftools view -H | wc -l
2610
\end{verbatim}

and how many variants will be excluded with QD < 2.0:

\begin{verbatim}
bcftools filter -i 'QD < 2.0' purged.vcf.gz | bcftools view -H | wc -l
15089
\end{verbatim}


To correctly translate the previous recommendation and combine the two criterions, one would be advised to use the logical '||' operator (OR) (rather than '\&\&' (AND) ) because one will want to throw out any variants that fail at least one criterion:

\begin{verbatim}
bcftools filter -i 'FS > 60 || QD < 2.0' purged.vcf.gz | bcftools view -H | wc -l
17273
\end{verbatim}


To actually remove these variants, simply use the -e option (--exclude) and create a new file:

\begin{verbatim}
bcftools filter -e 'FS > 60 || QD < 2.0' purged.vcf.gz | bgzip -c > newfile.vcf.gz
\end{verbatim}


Another reasonable hard filtering that can be done would be to exclude calls with poor quality and low read depth:

\begin{verbatim}
bcftools filter -e "QUAL < 20.0 && INFO/DP < 10" output.vcf.gz > filtered.vcf.gz
\end{verbatim}

A very intersting ressource to adequately applying other hard filters can be found at \href{https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants}{https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants}.




\subsection{Examining and Visualizing Alignment files}

At a moment you will want to visualize alignments files and see the correspondance between called variants found in VCF files and the final alignments. This will allow you to detect potential misaligned regions or other artifacts. Tablet \href{https://ics.hutton.ac.uk/tablet/download-tablet/}{https://ics.hutton.ac.uk/tablet/download-tablet/} and the Integrated Genome viewver \ref{https://software.broadinstitute.org/software/igv/}{https://software.broadinstitute.org/software/igv/} are good choices for such tasks.

\subsubsection{Tweaking the GFF3 file}

If you are interested to add a genome track representing annotated features, the Ensembl UMD 3.1 GFF3 file downloaded previously (subsection x) is your best option. However, as previously, chromosome names in this file differ from those present in the STAR genome and consequently from those present in our alignment files. With the command below, one can see the number of features annotated for each distinct identifier present in the GFF3 file:

\begin{verbatim}
cat Ensembl79_UMD3.1_genes.gff3 | cut -f 1 | uniq -cd
\end{verbatim}


%output of cat Ensembl79 gff3
\noindent output:
\begin{verbatim}
21287 GK000001.2
18819 GK000030.2
26235 GK000002.2
29793 GK000003.2
...
14 GJ058430.1
31 GJ058425.1
...
23 GJ059509.1
3 GJ058729.1
3 GJ060027.1
3 GJ058256.1
\end{verbatim}

Therefore,we need to replace the pattern used for the chromosome IDs (GK + 0000 + chromosome number) by solely the chromosome number. This could be done easily with sed:

\begin{verbatim}
sed -r s'/^GK[0]+([0-9]+).2/\1/'g Ensembl79_UMD3.1_genes.gff3 \
> Ensembl79_UMD3.1_genes_e.gff3
\end{verbatim}

Obvisouly it is good idea to inspect the edited file to ensure that the desired changes have been done properly.



\subsubsection{Inspecting reads from sample SRR5487372}



Here a screenshot of sample SRR5487372 in chromosome 11 between 

\input{figures/fig3}



bcftools view -s SRR5487372 -r 11:15063000-15068675  purged.vcf.gz

Figure 3 show an heterozygous call in a exonic region on chromosome 11.

In the upper panel we clearly can accumuilation of reads that correspond to exons from gene XX.







